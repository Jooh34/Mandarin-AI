Mandarin-AI
Alpha-Zero based Reinforcement Learning AI

## Features
  * Monte-Carlo Tree Search
  * Self-Play Simulation
  * Neural Network with Residual Block
  * Shared the parameters for the value and policy networks.
  * Node Recycling on MCTS
  * Data Augmentation with symmetry


## Othello 6x6

![mandarin-othello66](https://github.com/Jooh34/Mandarin-AI/assets/15865928/e302f762-d7ae-4f9a-9694-93fa000869d8)
![862](https://github.com/Jooh34/Mandarin-AI/assets/15865928/3da1f52b-8c47-4b93-87c8-637132e35c07)
*loss per training step*
![winrate_862](https://github.com/Jooh34/Mandarin-AI/assets/15865928/67e13f6a-4be5-4e7a-a9cd-55109c07a75f)
*win rate (vs randomplay) per epoch*


## Othello 8x8

I thought I almost win the game. But Mandarin-AI was confident of winning the game.
Let's see example.

https://github.com/Jooh34/Mandarin-AI/assets/15865928/43771a8d-3518-456c-b1e5-1bc0cbfa111a


![loss4292](https://github.com/Jooh34/Mandarin-AI/assets/15865928/95ecf310-b363-4666-8410-054f003b9bed)
*loss per training step*
![winrate_4292](https://github.com/Jooh34/Mandarin-AI/assets/15865928/77b65873-453f-428a-a111-ee81e5d2713c)
*win rate (vs randomplay) per epoch*
